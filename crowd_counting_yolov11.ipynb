{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Delta-K-rist/kai-ai-model/blob/main/crowd_counting_yolov11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Repo Git"
      ],
      "metadata": {
        "id": "6_zTgJ3NHvuV"
      },
      "id": "6_zTgJ3NHvuV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone own repo\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "github_username = 'Delta-K-rist'\n",
        "github_repo_name = 'kai-ai-model'\n",
        "\n",
        "github_token = userdata.get('GITHUB_TOKEN_DELTA')\n",
        "\n",
        "authenticated_github_url = f'https://{github_username}:{github_token}@github.com/{github_username}/{github_repo_name}.git'\n",
        "\n",
        "!git clone {authenticated_github_url}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTehAwcTHx0_",
        "outputId": "921bbf51-6727-47aa-acdb-f0475e45f3cb"
      },
      "id": "FTehAwcTHx0_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kai-ai-model'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 49 (delta 2), reused 8 (delta 2), pack-reused 38 (from 3)\u001b[K\n",
            "Receiving objects: 100% (49/49), 241.15 MiB | 22.01 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "jFgrx6ggsK2R"
      },
      "id": "jFgrx6ggsK2R"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ultralytics opencv-python-headless"
      ],
      "metadata": {
        "id": "VRFekfdosOUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab98df5-52f3-43e9-a93a-f8353e652ac2"
      },
      "id": "VRFekfdosOUF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ultralytics \"lap>=0.5.12\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XELADI8hfv5R",
        "outputId": "f6bb685e-f5d8-419d-f641-fda75e4eaa0e"
      },
      "id": "XELADI8hfv5R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.7 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Initialization (YOLOv11.m)"
      ],
      "metadata": {
        "id": "TX5rNgr1ssLM"
      },
      "id": "TX5rNgr1ssLM"
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# STEP 1.2: IMPORTS AND THE `initialize_model` FUNCTION\n",
        "# -----------------------------------------------------------------------------\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def initialize_model():\n",
        "    \"\"\"\n",
        "    Loads and initializes the YOLOv11 detection model.\n",
        "    The model object itself contains the tracking capabilities.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the initialized 'yolo' model.\n",
        "    \"\"\"\n",
        "    print(\"ğŸ§  Initializing YOLOv11 model...\")\n",
        "\n",
        "    # Load the YOLOv11 small model\n",
        "    # The model weights are downloaded automatically on the first run\n",
        "    yolo_model = YOLO('yolo11m.pt')\n",
        "\n",
        "    # Move model to GPU if available, otherwise CPU\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    yolo_model.to(device)\n",
        "    print(f\"YOLOv11 model loaded on {device}.\")\n",
        "\n",
        "    # We return it in a dictionary to keep our code structure consistent\n",
        "    # and easy to extend later if needed.\n",
        "    models = {\n",
        "        'yolo': yolo_model\n",
        "    }\n",
        "\n",
        "    return models"
      ],
      "metadata": {
        "id": "RDEsEPWeuQBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4eb90cd-e200-430c-f69b-66d77bbcf678"
      },
      "id": "RDEsEPWeuQBx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# STEP 1.3: TEST THE INITIALIZATION\n",
        "# -----------------------------------------------------------------------------\n",
        "# Call the function to get the initialized model\n",
        "ai_models = initialize_model()\n",
        "\n",
        "# Print the model's device to confirm it's loaded correctly\n",
        "# This should show 'cuda' if you're on a GPU environment, or 'cpu'\n",
        "print(\"\\nğŸ” Model loaded successfully and is running on:\", ai_models['yolo'].device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4NW-rZ9suu7",
        "outputId": "54cd6689-5f18-4adb-a2d4-5eb8391b8e2b"
      },
      "id": "Z4NW-rZ9suu7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  Initializing YOLOv11 model...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt to 'yolo11m.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 38.8MB 183.0MB/s 0.2s\n",
            "YOLOv11 model loaded on cuda.\n",
            "\n",
            "ğŸ” Model loaded successfully and is running on: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detection & Tracker"
      ],
      "metadata": {
        "id": "hytbmGuwuesu"
      },
      "id": "hytbmGuwuesu"
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# STEP 2.1 (UPDATED): THE CORE VIDEO PROCESSING FUNCTION\n",
        "# -----------------------------------------------------------------------------\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def process_video_with_tracker(video_path, model, output_dir):\n",
        "    \"\"\"\n",
        "    Processes a video, saves an annotated version with inference speed,\n",
        "    and collects performance data.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A list of person counts for each frame.\n",
        "        - The annotated frame with the highest person count.\n",
        "        - A list of inference times (in ms) for each frame.\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ“¹ Starting video processing and annotation for: {video_path}\")\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    cap.release()\n",
        "\n",
        "    output_video_path = os.path.join(output_dir, \"output_video.mp4\")\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    results_generator = model.track(source=video_path, persist=True, tracker=\"bytetrack.yaml\", classes=0, stream=True, verbose=False)\n",
        "\n",
        "    # --- NEW: List to store inference times ---\n",
        "    inference_times = []\n",
        "\n",
        "    frame_by_frame_counts = []\n",
        "    max_person_count = -1\n",
        "    annotated_max_frame = None\n",
        "    original_max_frame = None\n",
        "\n",
        "    confidences_at_max = []\n",
        "\n",
        "    for frame_results in results_generator:\n",
        "        annotated_frame = frame_results.plot(\n",
        "          line_width=1,  # Thinner bounding box\n",
        "          font_size=0.4  # Smaller font size\n",
        "        )\n",
        "\n",
        "        # --- NEW: Get inference time and calculate FPS ---\n",
        "        # The 'speed' attribute is a dict: {'preprocess': ms, 'inference': ms, 'postprocess': ms}\n",
        "        inference_time_ms = frame_results.speed['inference']\n",
        "        inference_times.append(inference_time_ms)\n",
        "\n",
        "        writer.write(annotated_frame)\n",
        "\n",
        "        if frame_results.boxes is not None and frame_results.boxes.id is not None:\n",
        "            current_count = len(frame_results.boxes.id)\n",
        "        else:\n",
        "            current_count = 0\n",
        "\n",
        "        frame_by_frame_counts.append(current_count)\n",
        "\n",
        "        if current_count > max_person_count:\n",
        "            max_person_count = current_count\n",
        "            annotated_max_frame = annotated_frame.copy()\n",
        "            original_max_frame = frame_results.orig_img.copy()\n",
        "\n",
        "            if frame_results.boxes.conf is not None:\n",
        "                confidences_at_max = frame_results.boxes.conf.tolist()\n",
        "\n",
        "    writer.release()\n",
        "    print(f\"âœ… Annotated video saved to {output_video_path}\")\n",
        "\n",
        "    # --- NEW: Return the list of inference times ---\n",
        "    return frame_by_frame_counts, annotated_max_frame, original_max_frame, inference_times, confidences_at_max"
      ],
      "metadata": {
        "id": "xSQGqj1buzRO"
      },
      "id": "xSQGqj1buzRO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# STEP 2.2 (UPDATED): THE SAVE RESULTS FUNCTION\n",
        "# -----------------------------------------------------------------------------\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "\n",
        "def save_analysis_summary(output_dir, counts, annotated_snapshot, original_snapshot, inference_times, video_basename, confidences_at_max):\n",
        "    \"\"\"\n",
        "    Calculates final metrics and saves the output files, including both snapshot versions.\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ’¾ Saving analysis to: {output_dir}\")\n",
        "\n",
        "    if video_basename == 'd_1':\n",
        "        gerbong_id = 'gerbong_3'\n",
        "    elif video_basename == 'm_1':\n",
        "        gerbong_id = 'gerbong_2'\n",
        "    elif video_basename == 's_2':\n",
        "        gerbong_id = 'gerbong_1'\n",
        "    else:\n",
        "        gerbong_id = 'gerbong_unknown' # Default case\n",
        "\n",
        "    avg_confidence = 0\n",
        "    if confidences_at_max:\n",
        "        avg_confidence = sum(confidences_at_max) / len(confidences_at_max)\n",
        "\n",
        "    # ... (all the calculation logic remains the same) ...\n",
        "    human_count = int(max(counts)) if counts else 0\n",
        "    MEDIUM_THRESHOLD = 15\n",
        "    DANGEROUS_THRESHOLD = 35\n",
        "    crowdness_level = \"Low Density\"\n",
        "    if human_count >= DANGEROUS_THRESHOLD:\n",
        "        crowdness_level = \"High Density\"\n",
        "    elif human_count >= MEDIUM_THRESHOLD:\n",
        "        crowdness_level = \"Medium Density\"\n",
        "    total_inference_seconds = sum(inference_times) / 1000.0\n",
        "    avg_inference_ms = sum(inference_times) / len(inference_times) if inference_times else 0\n",
        "\n",
        "    # --- MODIFIED: Save both snapshot images with new names ---\n",
        "    if annotated_snapshot is not None:\n",
        "        text = f\"Human Count: {human_count} ({crowdness_level})\"\n",
        "        cv2.putText(annotated_snapshot, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "        snapshot_path = os.path.join(output_dir, 'snapshot_annotated.jpg')\n",
        "        cv2.imwrite(snapshot_path, annotated_snapshot)\n",
        "        print(f\"ğŸ“¸ Annotated snapshot saved to {snapshot_path}\")\n",
        "\n",
        "    if original_snapshot is not None:\n",
        "        snapshot_path_orig = os.path.join(output_dir, 'snapshot_original.jpg')\n",
        "        cv2.imwrite(snapshot_path_orig, original_snapshot)\n",
        "        print(f\"ğŸ“¸ Original snapshot saved to {snapshot_path_orig}\")\n",
        "\n",
        "    # ... (JSON saving logic remains the same) ...\n",
        "    results_data = {\n",
        "        'gerbong_id': gerbong_id,\n",
        "        'max_human_count': human_count,\n",
        "        'confidence_score': round(avg_confidence, 2),\n",
        "        'crowdness_level': crowdness_level,\n",
        "        'performance': {\n",
        "            'total_inference_seconds': round(total_inference_seconds, 2),\n",
        "            'average_inference_ms': round(avg_inference_ms, 2),\n",
        "            'average_fps': round(1000 / avg_inference_ms, 1) if avg_inference_ms > 0 else 'inf'\n",
        "        },\n",
        "        'frame_count_data': counts\n",
        "    }\n",
        "    json_path = os.path.join(output_dir, 'results.json')\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(results_data, f, indent=4)\n",
        "    print(f\"ğŸ“„ JSON summary saved to {json_path}\")"
      ],
      "metadata": {
        "id": "tSYHBpx9vF0o"
      },
      "id": "tSYHBpx9vF0o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "GAuTZ9X8YUWn"
      },
      "id": "GAuTZ9X8YUWn"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def preprocess_video(input_path, output_path, target_fps=15, target_width=1280):\n",
        "    \"\"\"\n",
        "    Creates an optimized version of a video for faster model processing.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): Path to the original video file.\n",
        "        output_path (str): Path to save the new, preprocessed video.\n",
        "        target_fps (int): The desired frames per second.\n",
        "        target_width (int): The desired frame width. Height is scaled automatically.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the newly created video file.\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ”§ Starting preprocessing for {input_path}...\")\n",
        "\n",
        "    # 1. Open the original video\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return None\n",
        "\n",
        "    # 2. Get original video properties\n",
        "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # 3. Calculate resampling and resizing parameters\n",
        "    skip_interval = max(1, round(original_fps / target_fps))\n",
        "    aspect_ratio = original_height / original_width\n",
        "    target_height = int(target_width * aspect_ratio)\n",
        "\n",
        "    print(f\"Original: {original_width}x{original_height} @ {original_fps:.2f} FPS\")\n",
        "    print(f\"Target:   {target_width}x{target_height} @ {target_fps} FPS (keeping 1 in every {skip_interval} frames)\")\n",
        "\n",
        "    # 4. Initialize the Video Writer for the new video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(output_path, fourcc, target_fps, (target_width, target_height))\n",
        "\n",
        "    # 5. Loop, Resample, Resize, and Write\n",
        "    frame_number = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break # End of video\n",
        "\n",
        "        # Only process a frame if it's at the correct interval\n",
        "        if frame_number % skip_interval == 0:\n",
        "            # Resize the frame\n",
        "            resized_frame = cv2.resize(frame, (target_width, target_height))\n",
        "            # Write the resized frame to the new video\n",
        "            writer.write(resized_frame)\n",
        "\n",
        "        frame_number += 1\n",
        "\n",
        "    # 6. Finalize and clean up\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "\n",
        "    print(f\"âœ… Preprocessing complete. Optimized video saved to: {output_path}\")\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "2pBx87PIYWP0"
      },
      "id": "2pBx87PIYWP0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "qdPVPfz1vftR"
      },
      "id": "qdPVPfz1vftR"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from IPython.display import Image, display, Video\n",
        "import json\n",
        "\n",
        "INPUT_FOLDER = '/content/kai-ai-model/dataset/test/captured/new'\n",
        "BASE_OUTPUT_FOLDER = '/content/kai-ai-model/dataset/test result'\n",
        "os.makedirs(BASE_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "mp4_files = glob.glob(os.path.join(INPUT_FOLDER, '*.mp4')) + glob.glob(os.path.join(INPUT_FOLDER, '*.MP4'))\n",
        "mov_files = glob.glob(os.path.join(INPUT_FOLDER, '*.mov')) + glob.glob(os.path.join(INPUT_FOLDER, '*.MOV'))\n",
        "video_files = sorted(list(set(mp4_files + mov_files)))\n",
        "print(f\"Found {len(video_files)} videos to process.\")\n",
        "\n",
        "print(\"\\n--- INITIALIZING MODEL (ONCE) ---\")\n",
        "models = initialize_model()\n",
        "\n",
        "for original_video_path in video_files:\n",
        "    print(f\"\\n{'='*50}\\nğŸ¬ PROCESSING VIDEO: {os.path.basename(original_video_path)}\\n{'='*50}\")\n",
        "\n",
        "    video_basename = os.path.splitext(os.path.basename(original_video_path))[0]\n",
        "    final_output_dir = os.path.join(BASE_OUTPUT_FOLDER, video_basename)\n",
        "    os.makedirs(final_output_dir, exist_ok=True)\n",
        "\n",
        "    preprocessed_video_path = os.path.join(final_output_dir, 'preprocessed.mp4')\n",
        "\n",
        "    actual_video_to_process = preprocess_video(original_video_path, preprocessed_video_path)\n",
        "\n",
        "    if actual_video_to_process:\n",
        "        # --- MODIFIED: Unpack the new confidences_at_max list ---\n",
        "        counts, annotated_snap, original_snap, times, confidences_at_max = process_video_with_tracker(\n",
        "            actual_video_to_process,\n",
        "            models['yolo'],\n",
        "            final_output_dir\n",
        "        )\n",
        "\n",
        "        # --- MODIFIED: Pass the new arguments to the save function ---\n",
        "        save_analysis_summary(\n",
        "            final_output_dir,\n",
        "            counts,\n",
        "            annotated_snap,\n",
        "            original_snap,\n",
        "            times,\n",
        "            video_basename,\n",
        "            confidences_at_max\n",
        "        )\n",
        "        print(f\"âœ… SUCCESSFULLY PROCESSED: {os.path.basename(original_video_path)}\")\n",
        "    else:\n",
        "        print(f\"âŒ FAILED to preprocess: {os.path.basename(original_video_path)}\")\n",
        "\n",
        "print(f\"\\n\\n{'='*50}\\nğŸ‰ BATCH PROCESSING COMPLETE! ğŸ‰\\n{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcgMl4zjvhDW",
        "outputId": "79fa842a-43c8-40dd-d080-a4197478d644"
      },
      "id": "jcgMl4zjvhDW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 videos to process.\n",
            "\n",
            "--- INITIALIZING MODEL (ONCE) ---\n",
            "ğŸ§  Initializing YOLOv11 model...\n",
            "YOLOv11 model loaded on cuda.\n",
            "\n",
            "==================================================\n",
            "ğŸ¬ PROCESSING VIDEO: d_1.mp4\n",
            "==================================================\n",
            "ğŸ”§ Starting preprocessing for /content/kai-ai-model/dataset/test/captured/new/d_1.mp4...\n",
            "Original: 1280x720 @ 30.00 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "âœ… Preprocessing complete. Optimized video saved to: /content/kai-ai-model/dataset/test result/d_1/preprocessed.mp4\n",
            "ğŸ“¹ Starting video processing and annotation for: /content/kai-ai-model/dataset/test result/d_1/preprocessed.mp4\n",
            "âœ… Annotated video saved to /content/kai-ai-model/dataset/test result/d_1/output_video.mp4\n",
            "ğŸ’¾ Saving analysis to: /content/kai-ai-model/dataset/test result/d_1\n",
            "ğŸ“¸ Annotated snapshot saved to /content/kai-ai-model/dataset/test result/d_1/snapshot_annotated.jpg\n",
            "ğŸ“¸ Original snapshot saved to /content/kai-ai-model/dataset/test result/d_1/snapshot_original.jpg\n",
            "ğŸ“„ JSON summary saved to /content/kai-ai-model/dataset/test result/d_1/results.json\n",
            "âœ… SUCCESSFULLY PROCESSED: d_1.mp4\n",
            "\n",
            "==================================================\n",
            "ğŸ¬ PROCESSING VIDEO: d_2.mp4\n",
            "==================================================\n",
            "ğŸ”§ Starting preprocessing for /content/kai-ai-model/dataset/test/captured/new/d_2.mp4...\n",
            "Original: 1280x720 @ 30.00 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "âœ… Preprocessing complete. Optimized video saved to: /content/kai-ai-model/dataset/test result/d_2/preprocessed.mp4\n",
            "ğŸ“¹ Starting video processing and annotation for: /content/kai-ai-model/dataset/test result/d_2/preprocessed.mp4\n",
            "âœ… Annotated video saved to /content/kai-ai-model/dataset/test result/d_2/output_video.mp4\n",
            "ğŸ’¾ Saving analysis to: /content/kai-ai-model/dataset/test result/d_2\n",
            "ğŸ“¸ Annotated snapshot saved to /content/kai-ai-model/dataset/test result/d_2/snapshot_annotated.jpg\n",
            "ğŸ“¸ Original snapshot saved to /content/kai-ai-model/dataset/test result/d_2/snapshot_original.jpg\n",
            "ğŸ“„ JSON summary saved to /content/kai-ai-model/dataset/test result/d_2/results.json\n",
            "âœ… SUCCESSFULLY PROCESSED: d_2.mp4\n",
            "\n",
            "==================================================\n",
            "ğŸ¬ PROCESSING VIDEO: d_3.mp4\n",
            "==================================================\n",
            "ğŸ”§ Starting preprocessing for /content/kai-ai-model/dataset/test/captured/new/d_3.mp4...\n",
            "Original: 1280x720 @ 30.00 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "âœ… Preprocessing complete. Optimized video saved to: /content/kai-ai-model/dataset/test result/d_3/preprocessed.mp4\n",
            "ğŸ“¹ Starting video processing and annotation for: /content/kai-ai-model/dataset/test result/d_3/preprocessed.mp4\n",
            "âœ… Annotated video saved to /content/kai-ai-model/dataset/test result/d_3/output_video.mp4\n",
            "ğŸ’¾ Saving analysis to: /content/kai-ai-model/dataset/test result/d_3\n",
            "ğŸ“¸ Annotated snapshot saved to /content/kai-ai-model/dataset/test result/d_3/snapshot_annotated.jpg\n",
            "ğŸ“¸ Original snapshot saved to /content/kai-ai-model/dataset/test result/d_3/snapshot_original.jpg\n",
            "ğŸ“„ JSON summary saved to /content/kai-ai-model/dataset/test result/d_3/results.json\n",
            "âœ… SUCCESSFULLY PROCESSED: d_3.mp4\n",
            "\n",
            "==================================================\n",
            "ğŸ¬ PROCESSING VIDEO: m_1.MOV\n",
            "==================================================\n",
            "ğŸ”§ Starting preprocessing for /content/kai-ai-model/dataset/test/captured/new/m_1.MOV...\n",
            "Original: 1920x1080 @ 29.97 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "âœ… Preprocessing complete. Optimized video saved to: /content/kai-ai-model/dataset/test result/m_1/preprocessed.mp4\n",
            "ğŸ“¹ Starting video processing and annotation for: /content/kai-ai-model/dataset/test result/m_1/preprocessed.mp4\n",
            "âœ… Annotated video saved to /content/kai-ai-model/dataset/test result/m_1/output_video.mp4\n",
            "ğŸ’¾ Saving analysis to: /content/kai-ai-model/dataset/test result/m_1\n",
            "ğŸ“¸ Annotated snapshot saved to /content/kai-ai-model/dataset/test result/m_1/snapshot_annotated.jpg\n",
            "ğŸ“¸ Original snapshot saved to /content/kai-ai-model/dataset/test result/m_1/snapshot_original.jpg\n",
            "ğŸ“„ JSON summary saved to /content/kai-ai-model/dataset/test result/m_1/results.json\n",
            "âœ… SUCCESSFULLY PROCESSED: m_1.MOV\n",
            "\n",
            "==================================================\n",
            "ğŸ¬ PROCESSING VIDEO: m_2.MOV\n",
            "==================================================\n",
            "ğŸ”§ Starting preprocessing for /content/kai-ai-model/dataset/test/captured/new/m_2.MOV...\n",
            "Original: 1920x1080 @ 29.97 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "âœ… Preprocessing complete. Optimized video saved to: /content/kai-ai-model/dataset/test result/m_2/preprocessed.mp4\n",
            "ğŸ“¹ Starting video processing and annotation for: /content/kai-ai-model/dataset/test result/m_2/preprocessed.mp4\n",
            "âœ… Annotated video saved to /content/kai-ai-model/dataset/test result/m_2/output_video.mp4\n",
            "ğŸ’¾ Saving analysis to: /content/kai-ai-model/dataset/test result/m_2\n",
            "ğŸ“¸ Annotated snapshot saved to /content/kai-ai-model/dataset/test result/m_2/snapshot_annotated.jpg\n",
            "ğŸ“¸ Original snapshot saved to /content/kai-ai-model/dataset/test result/m_2/snapshot_original.jpg\n",
            "ğŸ“„ JSON summary saved to /content/kai-ai-model/dataset/test result/m_2/results.json\n",
            "âœ… SUCCESSFULLY PROCESSED: m_2.MOV\n",
            "\n",
            "==================================================\n",
            "ğŸ¬ PROCESSING VIDEO: m_3.mp4\n",
            "==================================================\n",
            "ğŸ”§ Starting preprocessing for /content/kai-ai-model/dataset/test/captured/new/m_3.mp4...\n",
            "Original: 1280x720 @ 29.66 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "âœ… Preprocessing complete. Optimized video saved to: /content/kai-ai-model/dataset/test result/m_3/preprocessed.mp4\n",
            "ğŸ“¹ Starting video processing and annotation for: /content/kai-ai-model/dataset/test result/m_3/preprocessed.mp4\n",
            "âœ… Annotated video saved to /content/kai-ai-model/dataset/test result/m_3/output_video.mp4\n",
            "ğŸ’¾ Saving analysis to: /content/kai-ai-model/dataset/test result/m_3\n",
            "ğŸ“¸ Annotated snapshot saved to /content/kai-ai-model/dataset/test result/m_3/snapshot_annotated.jpg\n",
            "ğŸ“¸ Original snapshot saved to /content/kai-ai-model/dataset/test result/m_3/snapshot_original.jpg\n",
            "ğŸ“„ JSON summary saved to /content/kai-ai-model/dataset/test result/m_3/results.json\n",
            "âœ… SUCCESSFULLY PROCESSED: m_3.mp4\n",
            "\n",
            "==================================================\n",
            "ğŸ¬ PROCESSING VIDEO: m_4.mp4\n",
            "==================================================\n",
            "ğŸ”§ Starting preprocessing for /content/kai-ai-model/dataset/test/captured/new/m_4.mp4...\n",
            "Original: 720x1280 @ 30.00 FPS\n",
            "Target:   1280x2275 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "âœ… Preprocessing complete. Optimized video saved to: /content/kai-ai-model/dataset/test result/m_4/preprocessed.mp4\n",
            "ğŸ“¹ Starting video processing and annotation for: /content/kai-ai-model/dataset/test result/m_4/preprocessed.mp4\n",
            "âœ… Annotated video saved to /content/kai-ai-model/dataset/test result/m_4/output_video.mp4\n",
            "ğŸ’¾ Saving analysis to: /content/kai-ai-model/dataset/test result/m_4\n",
            "ğŸ“¸ Annotated snapshot saved to /content/kai-ai-model/dataset/test result/m_4/snapshot_annotated.jpg\n",
            "ğŸ“¸ Original snapshot saved to /content/kai-ai-model/dataset/test result/m_4/snapshot_original.jpg\n",
            "ğŸ“„ JSON summary saved to /content/kai-ai-model/dataset/test result/m_4/results.json\n",
            "âœ… SUCCESSFULLY PROCESSED: m_4.mp4\n",
            "\n",
            "==================================================\n",
            "ğŸ¬ PROCESSING VIDEO: s_1.MOV\n",
            "==================================================\n",
            "ğŸ”§ Starting preprocessing for /content/kai-ai-model/dataset/test/captured/new/s_1.MOV...\n",
            "Original: 1920x1080 @ 30.01 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "âœ… Preprocessing complete. Optimized video saved to: /content/kai-ai-model/dataset/test result/s_1/preprocessed.mp4\n",
            "ğŸ“¹ Starting video processing and annotation for: /content/kai-ai-model/dataset/test result/s_1/preprocessed.mp4\n",
            "âœ… Annotated video saved to /content/kai-ai-model/dataset/test result/s_1/output_video.mp4\n",
            "ğŸ’¾ Saving analysis to: /content/kai-ai-model/dataset/test result/s_1\n",
            "ğŸ“¸ Annotated snapshot saved to /content/kai-ai-model/dataset/test result/s_1/snapshot_annotated.jpg\n",
            "ğŸ“¸ Original snapshot saved to /content/kai-ai-model/dataset/test result/s_1/snapshot_original.jpg\n",
            "ğŸ“„ JSON summary saved to /content/kai-ai-model/dataset/test result/s_1/results.json\n",
            "âœ… SUCCESSFULLY PROCESSED: s_1.MOV\n",
            "\n",
            "==================================================\n",
            "ğŸ¬ PROCESSING VIDEO: s_2.MOV\n",
            "==================================================\n",
            "ğŸ”§ Starting preprocessing for /content/kai-ai-model/dataset/test/captured/new/s_2.MOV...\n",
            "Original: 1920x1080 @ 29.97 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "âœ… Preprocessing complete. Optimized video saved to: /content/kai-ai-model/dataset/test result/s_2/preprocessed.mp4\n",
            "ğŸ“¹ Starting video processing and annotation for: /content/kai-ai-model/dataset/test result/s_2/preprocessed.mp4\n",
            "âœ… Annotated video saved to /content/kai-ai-model/dataset/test result/s_2/output_video.mp4\n",
            "ğŸ’¾ Saving analysis to: /content/kai-ai-model/dataset/test result/s_2\n",
            "ğŸ“¸ Annotated snapshot saved to /content/kai-ai-model/dataset/test result/s_2/snapshot_annotated.jpg\n",
            "ğŸ“¸ Original snapshot saved to /content/kai-ai-model/dataset/test result/s_2/snapshot_original.jpg\n",
            "ğŸ“„ JSON summary saved to /content/kai-ai-model/dataset/test result/s_2/results.json\n",
            "âœ… SUCCESSFULLY PROCESSED: s_2.MOV\n",
            "\n",
            "\n",
            "==================================================\n",
            "ğŸ‰ BATCH PROCESSING COMPLETE! ğŸ‰\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The '-r' flag means 'recursive' to include all subfolders\n",
        "!zip -r /content/results.zip \"/content/kai-ai-model/dataset/test result\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaqx5OWpo5iL",
        "outputId": "cf9a348b-8638-4033-d600-2019c975af85"
      },
      "id": "zaqx5OWpo5iL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/kai-ai-model/dataset/test result/ (stored 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_1/ (stored 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_1/results.json (deflated 91%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_1/output_video.mp4 (deflated 2%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_1/preprocessed.mp4 (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_1/snapshot_annotated.jpg (deflated 1%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_1/snapshot_original.jpg (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_3/ (stored 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_3/results.json (deflated 89%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_3/output_video.mp4 (deflated 2%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_3/preprocessed.mp4 (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_3/snapshot_annotated.jpg (deflated 1%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_3/snapshot_original.jpg (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/s_1/ (stored 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/s_1/results.json (deflated 91%)\n",
            "  adding: content/kai-ai-model/dataset/test result/s_1/output_video.mp4 (deflated 1%)\n",
            "  adding: content/kai-ai-model/dataset/test result/s_1/preprocessed.mp4 (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/s_1/snapshot_annotated.jpg (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/s_1/snapshot_original.jpg (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_4/ (stored 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_4/results.json (deflated 91%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_4/output_video.mp4 (deflated 1%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_4/preprocessed.mp4 (deflated 1%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_4/snapshot_annotated.jpg (deflated 5%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_4/snapshot_original.jpg (deflated 5%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_1/ (stored 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_1/results.json (deflated 91%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_1/output_video.mp4 (deflated 2%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_1/preprocessed.mp4 (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_1/snapshot_annotated.jpg (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_1/snapshot_original.jpg (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/s_2/ (stored 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/s_2/results.json (deflated 90%)\n",
            "  adding: content/kai-ai-model/dataset/test result/s_2/output_video.mp4 (deflated 1%)\n",
            "  adding: content/kai-ai-model/dataset/test result/s_2/preprocessed.mp4 (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/s_2/snapshot_annotated.jpg (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/s_2/snapshot_original.jpg (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_2/ (stored 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_2/results.json (deflated 90%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_2/output_video.mp4 (deflated 2%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_2/preprocessed.mp4 (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_2/snapshot_annotated.jpg (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/m_2/snapshot_original.jpg (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_2/ (stored 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_2/results.json (deflated 91%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_2/output_video.mp4 (deflated 3%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_2/preprocessed.mp4 (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_2/snapshot_annotated.jpg (deflated 1%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_2/snapshot_original.jpg (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_3/ (stored 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_3/results.json (deflated 92%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_3/output_video.mp4 (deflated 3%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_3/preprocessed.mp4 (deflated 0%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_3/snapshot_annotated.jpg (deflated 4%)\n",
            "  adding: content/kai-ai-model/dataset/test result/d_3/snapshot_original.jpg (deflated 5%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dump"
      ],
      "metadata": {
        "id": "V_J_gWi6TeTu"
      },
      "id": "V_J_gWi6TeTu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2396f2b2-0d82-4aab-847d-560412cf91e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2396f2b2-0d82-4aab-847d-560412cf91e9",
        "outputId": "50236663-cf89-4fd1-d6c5-181d72d424ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt to 'yolo11m.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 38.8MB 86.5MB/s 0.4s\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"yolo11m.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform object detection on video\n",
        "results = model(\"/content/kai-ai-model/dataset/captured/vid_1.mp4\", save=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgScSoASHr13",
        "outputId": "2c276e14-8910-48b9-dd3d-24cc13424d05"
      },
      "id": "dgScSoASHr13",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING âš ï¸ \n",
            "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "video 1/1 (frame 1/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 2044.9ms\n",
            "video 1/1 (frame 2/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 15.4ms\n",
            "video 1/1 (frame 3/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 11.8ms\n",
            "video 1/1 (frame 4/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 11.8ms\n",
            "video 1/1 (frame 5/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 11.8ms\n",
            "video 1/1 (frame 6/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 12.4ms\n",
            "video 1/1 (frame 7/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 13.8ms\n",
            "video 1/1 (frame 8/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 hair drier, 14.0ms\n",
            "video 1/1 (frame 9/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 11.9ms\n",
            "video 1/1 (frame 10/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 11.2ms\n",
            "video 1/1 (frame 11/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 12.3ms\n",
            "video 1/1 (frame 12/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 1 backpack, 12.6ms\n",
            "video 1/1 (frame 13/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 backpack, 12.1ms\n",
            "video 1/1 (frame 14/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 12.1ms\n",
            "video 1/1 (frame 15/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 12.9ms\n",
            "video 1/1 (frame 16/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 12.1ms\n",
            "video 1/1 (frame 17/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 11.7ms\n",
            "video 1/1 (frame 18/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 1 backpack, 11.4ms\n",
            "video 1/1 (frame 19/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 2 backpacks, 12.9ms\n",
            "video 1/1 (frame 20/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 11.7ms\n",
            "video 1/1 (frame 21/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 13.0ms\n",
            "video 1/1 (frame 22/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 17.2ms\n",
            "video 1/1 (frame 23/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 13.5ms\n",
            "video 1/1 (frame 24/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 13.5ms\n",
            "video 1/1 (frame 25/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 1 backpack, 12.8ms\n",
            "video 1/1 (frame 26/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 12.7ms\n",
            "video 1/1 (frame 27/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 13.6ms\n",
            "video 1/1 (frame 28/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 11.0ms\n",
            "video 1/1 (frame 29/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 12.3ms\n",
            "video 1/1 (frame 30/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 14.3ms\n",
            "video 1/1 (frame 31/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 13.5ms\n",
            "video 1/1 (frame 32/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 13.8ms\n",
            "video 1/1 (frame 33/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 14.2ms\n",
            "video 1/1 (frame 34/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 14.9ms\n",
            "video 1/1 (frame 35/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 12.6ms\n",
            "video 1/1 (frame 36/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 13.7ms\n",
            "video 1/1 (frame 37/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 13.4ms\n",
            "video 1/1 (frame 38/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 11.8ms\n",
            "video 1/1 (frame 39/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 11.2ms\n",
            "video 1/1 (frame 40/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 12.3ms\n",
            "video 1/1 (frame 41/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 12.2ms\n",
            "video 1/1 (frame 42/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 11.5ms\n",
            "video 1/1 (frame 43/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 12.8ms\n",
            "video 1/1 (frame 44/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 13.5ms\n",
            "video 1/1 (frame 45/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 13.5ms\n",
            "video 1/1 (frame 46/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 13.5ms\n",
            "video 1/1 (frame 47/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 13.5ms\n",
            "video 1/1 (frame 48/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 15.1ms\n",
            "video 1/1 (frame 49/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 14.1ms\n",
            "video 1/1 (frame 50/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 13.8ms\n",
            "video 1/1 (frame 51/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 14.0ms\n",
            "video 1/1 (frame 52/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 13.4ms\n",
            "video 1/1 (frame 53/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 14.0ms\n",
            "video 1/1 (frame 54/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 14.0ms\n",
            "video 1/1 (frame 55/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 14.2ms\n",
            "video 1/1 (frame 56/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 13.6ms\n",
            "video 1/1 (frame 57/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 13.7ms\n",
            "video 1/1 (frame 58/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 14.0ms\n",
            "video 1/1 (frame 59/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 14.1ms\n",
            "video 1/1 (frame 60/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 13.8ms\n",
            "video 1/1 (frame 61/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 1 teddy bear, 13.8ms\n",
            "video 1/1 (frame 62/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 15.2ms\n",
            "video 1/1 (frame 63/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 14.2ms\n",
            "video 1/1 (frame 64/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 teddy bear, 14.3ms\n",
            "video 1/1 (frame 65/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 14.5ms\n",
            "video 1/1 (frame 66/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 13.9ms\n",
            "video 1/1 (frame 67/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 14.4ms\n",
            "video 1/1 (frame 68/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 15.2ms\n",
            "video 1/1 (frame 69/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 14.5ms\n",
            "video 1/1 (frame 70/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 14.1ms\n",
            "video 1/1 (frame 71/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 14.5ms\n",
            "video 1/1 (frame 72/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 23.1ms\n",
            "video 1/1 (frame 73/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 14.3ms\n",
            "video 1/1 (frame 74/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 14.1ms\n",
            "video 1/1 (frame 75/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 14.5ms\n",
            "video 1/1 (frame 76/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 14.4ms\n",
            "video 1/1 (frame 77/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 14.8ms\n",
            "video 1/1 (frame 78/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 14.9ms\n",
            "video 1/1 (frame 79/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 14.4ms\n",
            "video 1/1 (frame 80/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 14.8ms\n",
            "video 1/1 (frame 81/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 15.2ms\n",
            "video 1/1 (frame 82/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 15.2ms\n",
            "video 1/1 (frame 83/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 15.0ms\n",
            "video 1/1 (frame 84/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 14.7ms\n",
            "video 1/1 (frame 85/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 14.7ms\n",
            "video 1/1 (frame 86/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 14.7ms\n",
            "video 1/1 (frame 87/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 14.7ms\n",
            "video 1/1 (frame 88/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 14.7ms\n",
            "video 1/1 (frame 89/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 1 backpack, 16.0ms\n",
            "video 1/1 (frame 90/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 backpack, 15.2ms\n",
            "video 1/1 (frame 91/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 15.2ms\n",
            "video 1/1 (frame 92/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 16.1ms\n",
            "video 1/1 (frame 93/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 14.8ms\n",
            "video 1/1 (frame 94/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 14.8ms\n",
            "video 1/1 (frame 95/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 backpack, 15.6ms\n",
            "video 1/1 (frame 96/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 20.9ms\n",
            "video 1/1 (frame 97/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 15.1ms\n",
            "video 1/1 (frame 98/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 15.0ms\n",
            "video 1/1 (frame 99/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 15.5ms\n",
            "video 1/1 (frame 100/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 15.5ms\n",
            "video 1/1 (frame 101/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 16.4ms\n",
            "video 1/1 (frame 102/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 15.1ms\n",
            "video 1/1 (frame 103/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 15.6ms\n",
            "video 1/1 (frame 104/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 15.9ms\n",
            "video 1/1 (frame 105/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 15.8ms\n",
            "video 1/1 (frame 106/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 16.3ms\n",
            "video 1/1 (frame 107/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 19.3ms\n",
            "video 1/1 (frame 108/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 17.0ms\n",
            "video 1/1 (frame 109/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 16.2ms\n",
            "video 1/1 (frame 110/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 16.0ms\n",
            "video 1/1 (frame 111/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 teddy bear, 15.6ms\n",
            "video 1/1 (frame 112/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 teddy bear, 16.4ms\n",
            "video 1/1 (frame 113/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 16.2ms\n",
            "video 1/1 (frame 114/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 teddy bear, 15.9ms\n",
            "video 1/1 (frame 115/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 15.7ms\n",
            "video 1/1 (frame 116/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 15.7ms\n",
            "video 1/1 (frame 117/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 15.7ms\n",
            "video 1/1 (frame 118/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 17.3ms\n",
            "video 1/1 (frame 119/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 17.2ms\n",
            "video 1/1 (frame 120/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 15.8ms\n",
            "video 1/1 (frame 121/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 16.7ms\n",
            "video 1/1 (frame 122/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 16.7ms\n",
            "video 1/1 (frame 123/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 16.5ms\n",
            "video 1/1 (frame 124/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 16.1ms\n",
            "video 1/1 (frame 125/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 cell phone, 16.6ms\n",
            "video 1/1 (frame 126/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 16.6ms\n",
            "video 1/1 (frame 127/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 16.4ms\n",
            "video 1/1 (frame 128/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 17.6ms\n",
            "video 1/1 (frame 129/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 16.3ms\n",
            "video 1/1 (frame 130/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 16.1ms\n",
            "video 1/1 (frame 131/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 16.2ms\n",
            "video 1/1 (frame 132/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 17 persons, 1 backpack, 1 scissors, 17.2ms\n",
            "video 1/1 (frame 133/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 1 backpack, 1 scissors, 16.1ms\n",
            "video 1/1 (frame 134/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 1 scissors, 16.7ms\n",
            "video 1/1 (frame 135/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 1 scissors, 16.7ms\n",
            "video 1/1 (frame 136/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 1 scissors, 16.6ms\n",
            "video 1/1 (frame 137/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 1 scissors, 16.6ms\n",
            "video 1/1 (frame 138/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 1 scissors, 16.4ms\n",
            "video 1/1 (frame 139/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 1 backpack, 1 scissors, 18.6ms\n",
            "video 1/1 (frame 140/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 16.4ms\n",
            "video 1/1 (frame 141/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 1 backpack, 16.8ms\n",
            "video 1/1 (frame 142/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 143/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 144/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 145/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 1 hair drier, 17.0ms\n",
            "video 1/1 (frame 146/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 147/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 1 hair drier, 17.0ms\n",
            "video 1/1 (frame 148/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 2 hair driers, 17.0ms\n",
            "video 1/1 (frame 149/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 2 hair driers, 17.1ms\n",
            "video 1/1 (frame 150/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 hair drier, 17.2ms\n",
            "video 1/1 (frame 151/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 17.2ms\n",
            "video 1/1 (frame 152/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 19.3ms\n",
            "video 1/1 (frame 153/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 17.2ms\n",
            "video 1/1 (frame 154/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 17.2ms\n",
            "video 1/1 (frame 155/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 17.3ms\n",
            "video 1/1 (frame 156/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 17.2ms\n",
            "video 1/1 (frame 157/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 17.3ms\n",
            "video 1/1 (frame 158/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 17.2ms\n",
            "video 1/1 (frame 159/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 1 hair drier, 17.3ms\n",
            "video 1/1 (frame 160/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 23.5ms\n",
            "video 1/1 (frame 161/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 1 scissors, 17.9ms\n",
            "video 1/1 (frame 162/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 1 scissors, 21.0ms\n",
            "video 1/1 (frame 163/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 1 scissors, 18.0ms\n",
            "video 1/1 (frame 164/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 22.9ms\n",
            "video 1/1 (frame 165/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 23.5ms\n",
            "video 1/1 (frame 166/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 21.7ms\n",
            "video 1/1 (frame 167/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 23.0ms\n",
            "video 1/1 (frame 168/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 22.9ms\n",
            "video 1/1 (frame 169/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 2 backpacks, 22.4ms\n",
            "video 1/1 (frame 170/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 21.4ms\n",
            "video 1/1 (frame 171/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 22.7ms\n",
            "video 1/1 (frame 172/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 25.2ms\n",
            "video 1/1 (frame 173/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 20.8ms\n",
            "video 1/1 (frame 174/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 21.9ms\n",
            "video 1/1 (frame 175/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 20.9ms\n",
            "video 1/1 (frame 176/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 21.2ms\n",
            "video 1/1 (frame 177/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 19.8ms\n",
            "video 1/1 (frame 178/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 2 backpacks, 20.6ms\n",
            "video 1/1 (frame 179/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 22.5ms\n",
            "video 1/1 (frame 180/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 25.6ms\n",
            "video 1/1 (frame 181/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 22.2ms\n",
            "video 1/1 (frame 182/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 24.9ms\n",
            "video 1/1 (frame 183/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 20.5ms\n",
            "video 1/1 (frame 184/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 2 backpacks, 21.8ms\n",
            "video 1/1 (frame 185/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 backpack, 1 scissors, 21.8ms\n",
            "video 1/1 (frame 186/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 23.6ms\n",
            "video 1/1 (frame 187/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 25.3ms\n",
            "video 1/1 (frame 188/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 25.1ms\n",
            "video 1/1 (frame 189/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 26.4ms\n",
            "video 1/1 (frame 190/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 27.6ms\n",
            "video 1/1 (frame 191/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 1 backpack, 26.1ms\n",
            "video 1/1 (frame 192/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 1 backpack, 22.3ms\n",
            "video 1/1 (frame 193/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 1 backpack, 23.6ms\n",
            "video 1/1 (frame 194/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 22.4ms\n",
            "video 1/1 (frame 195/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 22.0ms\n",
            "video 1/1 (frame 196/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 21.9ms\n",
            "video 1/1 (frame 197/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 21.6ms\n",
            "video 1/1 (frame 198/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 21.6ms\n",
            "video 1/1 (frame 199/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 22.1ms\n",
            "video 1/1 (frame 200/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 21.2ms\n",
            "video 1/1 (frame 201/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 20.9ms\n",
            "video 1/1 (frame 202/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 21.6ms\n",
            "video 1/1 (frame 203/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 21.3ms\n",
            "video 1/1 (frame 204/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 20.6ms\n",
            "video 1/1 (frame 205/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 19.7ms\n",
            "video 1/1 (frame 206/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 19.8ms\n",
            "video 1/1 (frame 207/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 19.7ms\n",
            "video 1/1 (frame 208/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 19.7ms\n",
            "video 1/1 (frame 209/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 2 backpacks, 17.8ms\n",
            "video 1/1 (frame 210/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 2 backpacks, 21.8ms\n",
            "video 1/1 (frame 211/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 19.2ms\n",
            "video 1/1 (frame 212/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 18.3ms\n",
            "video 1/1 (frame 213/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 17.7ms\n",
            "video 1/1 (frame 214/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 2 backpacks, 17.7ms\n",
            "video 1/1 (frame 215/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 17.7ms\n",
            "video 1/1 (frame 216/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 17.7ms\n",
            "video 1/1 (frame 217/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 17.7ms\n",
            "video 1/1 (frame 218/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 17.7ms\n",
            "video 1/1 (frame 219/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 handbag, 17.7ms\n",
            "video 1/1 (frame 220/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 handbag, 17.7ms\n",
            "video 1/1 (frame 221/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 17.7ms\n",
            "video 1/1 (frame 222/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 17.7ms\n",
            "video 1/1 (frame 223/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 18.0ms\n",
            "video 1/1 (frame 224/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 17.7ms\n",
            "video 1/1 (frame 225/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 17.7ms\n",
            "video 1/1 (frame 226/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 18.5ms\n",
            "video 1/1 (frame 227/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 17.5ms\n",
            "video 1/1 (frame 228/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 17.5ms\n",
            "video 1/1 (frame 229/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 17.4ms\n",
            "video 1/1 (frame 230/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 17.5ms\n",
            "video 1/1 (frame 231/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 17.5ms\n",
            "video 1/1 (frame 232/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 17.5ms\n",
            "video 1/1 (frame 233/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 17.5ms\n",
            "video 1/1 (frame 234/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 17 persons, 17.5ms\n",
            "video 1/1 (frame 235/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 17.5ms\n",
            "video 1/1 (frame 236/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 17.5ms\n",
            "video 1/1 (frame 237/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 18 persons, 17.8ms\n",
            "video 1/1 (frame 238/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 239/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 240/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 241/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 3 backpacks, 17.5ms\n",
            "video 1/1 (frame 242/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 243/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 244/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 245/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 246/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 247/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 16 persons, 1 backpack, 17.7ms\n",
            "video 1/1 (frame 248/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 2 backpacks, 17.5ms\n",
            "video 1/1 (frame 249/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 250/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 251/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 252/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 253/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 254/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 backpack, 18.6ms\n",
            "video 1/1 (frame 255/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 256/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 17.5ms\n",
            "video 1/1 (frame 257/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 17.7ms\n",
            "video 1/1 (frame 258/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 17.5ms\n",
            "video 1/1 (frame 259/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 17.5ms\n",
            "video 1/1 (frame 260/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 18.7ms\n",
            "video 1/1 (frame 261/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 18.7ms\n",
            "video 1/1 (frame 262/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 18.1ms\n",
            "video 1/1 (frame 263/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 17.0ms\n",
            "video 1/1 (frame 264/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 18.2ms\n",
            "video 1/1 (frame 265/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 17.0ms\n",
            "video 1/1 (frame 266/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 267/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 268/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 269/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 17.0ms\n",
            "video 1/1 (frame 270/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 17.1ms\n",
            "video 1/1 (frame 271/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 17.0ms\n",
            "video 1/1 (frame 272/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 17.1ms\n",
            "video 1/1 (frame 273/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 cell phone, 17.0ms\n",
            "video 1/1 (frame 274/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 2 backpacks, 17.0ms\n",
            "video 1/1 (frame 275/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 17.7ms\n",
            "video 1/1 (frame 276/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 15 persons, 1 backpack, 21.6ms\n",
            "video 1/1 (frame 277/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 278/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 279/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 17.0ms\n",
            "video 1/1 (frame 280/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 281/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 282/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 283/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 284/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 285/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 17.1ms\n",
            "video 1/1 (frame 286/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 17.0ms\n",
            "video 1/1 (frame 287/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 17.0ms\n",
            "video 1/1 (frame 288/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 17.0ms\n",
            "video 1/1 (frame 289/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 17.9ms\n",
            "video 1/1 (frame 290/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 17.1ms\n",
            "video 1/1 (frame 291/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 14 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 292/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 1 backpack, 17.0ms\n",
            "video 1/1 (frame 293/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 backpack, 16.8ms\n",
            "video 1/1 (frame 294/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 19.2ms\n",
            "video 1/1 (frame 295/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 16.8ms\n",
            "video 1/1 (frame 296/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 16.9ms\n",
            "video 1/1 (frame 297/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 16.9ms\n",
            "video 1/1 (frame 298/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 24.1ms\n",
            "video 1/1 (frame 299/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 16.8ms\n",
            "video 1/1 (frame 300/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 16.8ms\n",
            "video 1/1 (frame 301/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 13 persons, 17.3ms\n",
            "video 1/1 (frame 302/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 16.8ms\n",
            "video 1/1 (frame 303/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 12 persons, 1 scissors, 16.8ms\n",
            "video 1/1 (frame 304/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 16.8ms\n",
            "video 1/1 (frame 305/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 16.9ms\n",
            "video 1/1 (frame 306/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 backpack, 16.8ms\n",
            "video 1/1 (frame 307/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 handbag, 16.9ms\n",
            "video 1/1 (frame 308/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 handbag, 16.8ms\n",
            "video 1/1 (frame 309/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 handbag, 16.8ms\n",
            "video 1/1 (frame 310/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 16.8ms\n",
            "video 1/1 (frame 311/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 1 backpack, 1 cell phone, 16.8ms\n",
            "video 1/1 (frame 312/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 8 persons, 1 backpack, 16.8ms\n",
            "video 1/1 (frame 313/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 7 persons, 1 backpack, 1 handbag, 17.5ms\n",
            "video 1/1 (frame 314/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 6 persons, 2 backpacks, 16.8ms\n",
            "video 1/1 (frame 315/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 16.8ms\n",
            "video 1/1 (frame 316/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 1 chair, 17.2ms\n",
            "video 1/1 (frame 317/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 8 persons, 16.9ms\n",
            "video 1/1 (frame 318/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 16.8ms\n",
            "video 1/1 (frame 319/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 16.9ms\n",
            "video 1/1 (frame 320/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 1 cell phone, 16.8ms\n",
            "video 1/1 (frame 321/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 7 persons, 1 backpack, 18.7ms\n",
            "video 1/1 (frame 322/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 1 handbag, 16.9ms\n",
            "video 1/1 (frame 323/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 8 persons, 16.8ms\n",
            "video 1/1 (frame 324/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 7 persons, 16.8ms\n",
            "video 1/1 (frame 325/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 16.8ms\n",
            "video 1/1 (frame 326/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 8 persons, 1 chair, 17.4ms\n",
            "video 1/1 (frame 327/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 8 persons, 17.1ms\n",
            "video 1/1 (frame 328/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 7 persons, 1 handbag, 16.9ms\n",
            "video 1/1 (frame 329/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 8 persons, 1 backpack, 16.9ms\n",
            "video 1/1 (frame 330/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 1 handbag, 16.9ms\n",
            "video 1/1 (frame 331/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 11 persons, 1 backpack, 16.9ms\n",
            "video 1/1 (frame 332/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 1 backpack, 16.6ms\n",
            "video 1/1 (frame 333/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 8 persons, 1 backpack, 16.8ms\n",
            "video 1/1 (frame 334/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 8 persons, 1 backpack, 16.4ms\n",
            "video 1/1 (frame 335/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 1 backpack, 17.4ms\n",
            "video 1/1 (frame 336/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 1 backpack, 16.3ms\n",
            "video 1/1 (frame 337/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 8 persons, 1 backpack, 16.3ms\n",
            "video 1/1 (frame 338/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 8 persons, 2 backpacks, 16.2ms\n",
            "video 1/1 (frame 339/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 1 backpack, 16.2ms\n",
            "video 1/1 (frame 340/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 7 persons, 1 backpack, 16.2ms\n",
            "video 1/1 (frame 341/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 8 persons, 16.5ms\n",
            "video 1/1 (frame 342/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 6 persons, 17.7ms\n",
            "video 1/1 (frame 343/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 6 persons, 15.6ms\n",
            "video 1/1 (frame 344/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 6 persons, 19.4ms\n",
            "video 1/1 (frame 345/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 8 persons, 1 backpack, 15.6ms\n",
            "video 1/1 (frame 346/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 1 backpack, 1 chair, 15.6ms\n",
            "video 1/1 (frame 347/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 3 backpacks, 1 chair, 15.6ms\n",
            "video 1/1 (frame 348/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 2 backpacks, 1 chair, 16.8ms\n",
            "video 1/1 (frame 349/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 9 persons, 1 backpack, 15.6ms\n",
            "video 1/1 (frame 350/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 1 backpack, 15.6ms\n",
            "video 1/1 (frame 351/351) /content/kai-ai-model/dataset/captured/vid_1.mp4: 384x640 10 persons, 1 backpack, 15.5ms\n",
            "Speed: 3.2ms preprocess, 22.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184eafbe-b3c6-469d-8e39-6de069b3feec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "184eafbe-b3c6-469d-8e39-6de069b3feec",
        "outputId": "7521b24a-c41c-4be0-b870-4c40d2004802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "WARNING âš ï¸ inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "video 1/1 (frame 1/188) /content/p.mp4: 384x640 10 persons, 24.4ms\n",
            "video 1/1 (frame 2/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 3/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 4/188) /content/p.mp4: 384x640 10 persons, 24.4ms\n",
            "video 1/1 (frame 5/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 6/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 7/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 8/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 9/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 10/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 11/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 12/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 13/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 14/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 15/188) /content/p.mp4: 384x640 10 persons, 24.9ms\n",
            "video 1/1 (frame 16/188) /content/p.mp4: 384x640 11 persons, 25.1ms\n",
            "video 1/1 (frame 17/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 18/188) /content/p.mp4: 384x640 10 persons, 24.4ms\n",
            "video 1/1 (frame 19/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 20/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 21/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 22/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 23/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 24/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 25/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 26/188) /content/p.mp4: 384x640 10 persons, 24.7ms\n",
            "video 1/1 (frame 27/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 28/188) /content/p.mp4: 384x640 10 persons, 24.9ms\n",
            "video 1/1 (frame 29/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 30/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 31/188) /content/p.mp4: 384x640 10 persons, 28.2ms\n",
            "video 1/1 (frame 32/188) /content/p.mp4: 384x640 11 persons, 25.8ms\n",
            "video 1/1 (frame 33/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 34/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 35/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 36/188) /content/p.mp4: 384x640 11 persons, 25.3ms\n",
            "video 1/1 (frame 37/188) /content/p.mp4: 384x640 11 persons, 30.1ms\n",
            "video 1/1 (frame 38/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 39/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 40/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 41/188) /content/p.mp4: 384x640 9 persons, 25.3ms\n",
            "video 1/1 (frame 42/188) /content/p.mp4: 384x640 9 persons, 24.4ms\n",
            "video 1/1 (frame 43/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 44/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 45/188) /content/p.mp4: 384x640 10 persons, 24.4ms\n",
            "video 1/1 (frame 46/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 47/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 48/188) /content/p.mp4: 384x640 12 persons, 24.3ms\n",
            "video 1/1 (frame 49/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 50/188) /content/p.mp4: 384x640 12 persons, 24.3ms\n",
            "video 1/1 (frame 51/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 52/188) /content/p.mp4: 384x640 13 persons, 24.5ms\n",
            "video 1/1 (frame 53/188) /content/p.mp4: 384x640 14 persons, 24.3ms\n",
            "video 1/1 (frame 54/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 55/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 56/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 57/188) /content/p.mp4: 384x640 12 persons, 24.3ms\n",
            "video 1/1 (frame 58/188) /content/p.mp4: 384x640 12 persons, 24.3ms\n",
            "video 1/1 (frame 59/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 60/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 61/188) /content/p.mp4: 384x640 12 persons, 24.9ms\n",
            "video 1/1 (frame 62/188) /content/p.mp4: 384x640 12 persons, 24.3ms\n",
            "video 1/1 (frame 63/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 64/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 65/188) /content/p.mp4: 384x640 12 persons, 24.3ms\n",
            "video 1/1 (frame 66/188) /content/p.mp4: 384x640 12 persons, 24.3ms\n",
            "video 1/1 (frame 67/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 68/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 69/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 70/188) /content/p.mp4: 384x640 12 persons, 24.3ms\n",
            "video 1/1 (frame 71/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 72/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 73/188) /content/p.mp4: 384x640 12 persons, 24.3ms\n",
            "video 1/1 (frame 74/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 75/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 76/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 77/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 78/188) /content/p.mp4: 384x640 11 persons, 26.0ms\n",
            "video 1/1 (frame 79/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 80/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 81/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 82/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 83/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 84/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 85/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 86/188) /content/p.mp4: 384x640 10 persons, 24.7ms\n",
            "video 1/1 (frame 87/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 88/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 89/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 90/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 91/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 92/188) /content/p.mp4: 384x640 10 persons, 28.6ms\n",
            "video 1/1 (frame 93/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 94/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 95/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 96/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 97/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 98/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 99/188) /content/p.mp4: 384x640 13 persons, 24.3ms\n",
            "video 1/1 (frame 100/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 101/188) /content/p.mp4: 384x640 11 persons, 24.5ms\n",
            "video 1/1 (frame 102/188) /content/p.mp4: 384x640 12 persons, 24.3ms\n",
            "video 1/1 (frame 103/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 104/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 105/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 106/188) /content/p.mp4: 384x640 10 persons, 26.3ms\n",
            "video 1/1 (frame 107/188) /content/p.mp4: 384x640 9 persons, 24.4ms\n",
            "video 1/1 (frame 108/188) /content/p.mp4: 384x640 8 persons, 24.4ms\n",
            "video 1/1 (frame 109/188) /content/p.mp4: 384x640 9 persons, 24.4ms\n",
            "video 1/1 (frame 110/188) /content/p.mp4: 384x640 9 persons, 24.4ms\n",
            "video 1/1 (frame 111/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 112/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 113/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 114/188) /content/p.mp4: 384x640 8 persons, 24.4ms\n",
            "video 1/1 (frame 115/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 116/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 117/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 118/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 119/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 120/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 121/188) /content/p.mp4: 384x640 6 persons, 24.4ms\n",
            "video 1/1 (frame 122/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 123/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 124/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 125/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 126/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 127/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 128/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 129/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 130/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 131/188) /content/p.mp4: 384x640 4 persons, 24.3ms\n",
            "video 1/1 (frame 132/188) /content/p.mp4: 384x640 4 persons, 24.3ms\n",
            "video 1/1 (frame 133/188) /content/p.mp4: 384x640 4 persons, 24.3ms\n",
            "video 1/1 (frame 134/188) /content/p.mp4: 384x640 4 persons, 24.3ms\n",
            "video 1/1 (frame 135/188) /content/p.mp4: 384x640 4 persons, 24.4ms\n",
            "video 1/1 (frame 136/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 137/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 138/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 139/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 140/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 141/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 142/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 143/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 144/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 145/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 146/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 147/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 148/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 149/188) /content/p.mp4: 384x640 6 persons, 24.4ms\n",
            "video 1/1 (frame 150/188) /content/p.mp4: 384x640 6 persons, 25.4ms\n",
            "video 1/1 (frame 151/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 152/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 153/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 154/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 155/188) /content/p.mp4: 384x640 7 persons, 24.6ms\n",
            "video 1/1 (frame 156/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 157/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 158/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 159/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 160/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 161/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 162/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 163/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 164/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 165/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 166/188) /content/p.mp4: 384x640 8 persons, 24.4ms\n",
            "video 1/1 (frame 167/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 168/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 169/188) /content/p.mp4: 384x640 9 persons, 43.9ms\n",
            "video 1/1 (frame 170/188) /content/p.mp4: 384x640 10 persons, 32.8ms\n",
            "video 1/1 (frame 171/188) /content/p.mp4: 384x640 10 persons, 24.4ms\n",
            "video 1/1 (frame 172/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 173/188) /content/p.mp4: 384x640 11 persons, 26.0ms\n",
            "video 1/1 (frame 174/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 175/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 176/188) /content/p.mp4: 384x640 11 persons, 24.3ms\n",
            "video 1/1 (frame 177/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 178/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 179/188) /content/p.mp4: 384x640 9 persons, 42.9ms\n",
            "video 1/1 (frame 180/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 181/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 182/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 183/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 184/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 185/188) /content/p.mp4: 384x640 12 persons, 24.3ms\n",
            "video 1/1 (frame 186/188) /content/p.mp4: 384x640 12 persons, 24.3ms\n",
            "video 1/1 (frame 187/188) /content/p.mp4: 384x640 13 persons, 62.9ms\n",
            "video 1/1 (frame 188/188) /content/p.mp4: 384x640 14 persons, 47.7ms\n",
            "Speed: 3.7ms preprocess, 25.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "results = model(\"/content/p.mp4\",classes=0, save=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Track"
      ],
      "metadata": {
        "id": "MPoiypbfUcH4"
      },
      "id": "MPoiypbfUcH4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80d6b5d5-2e88-4387-8296-de2da681d25e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80d6b5d5-2e88-4387-8296-de2da681d25e",
        "outputId": "5a8d3f83-98c0-4ffd-a406-e712c4439be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lapx>=0.5.2'] not found, attempting AutoUpdate...\n",
            "Collecting lapx>=0.5.2\n",
            "  Downloading lapx-0.5.11-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from lapx>=0.5.2) (1.26.4)\n",
            "Downloading lapx-0.5.11-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.7/1.7 MB 118.2 MB/s eta 0:00:00\n",
            "Installing collected packages: lapx\n",
            "Successfully installed lapx-0.5.11\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 3.4s, installed 1 package: ['lapx>=0.5.2']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m âš ï¸ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "WARNING âš ï¸ inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
            "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
            "\n",
            "Example:\n",
            "    results = model(source=..., stream=True)  # generator of Results objects\n",
            "    for r in results:\n",
            "        boxes = r.boxes  # Boxes object for bbox outputs\n",
            "        masks = r.masks  # Masks object for segment masks outputs\n",
            "        probs = r.probs  # Class probabilities for classification outputs\n",
            "\n",
            "video 1/1 (frame 1/188) /content/p.mp4: 384x640 9 persons, 24.4ms\n",
            "video 1/1 (frame 2/188) /content/p.mp4: 384x640 9 persons, 24.4ms\n",
            "video 1/1 (frame 3/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 4/188) /content/p.mp4: 384x640 9 persons, 25.2ms\n",
            "video 1/1 (frame 5/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 6/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 7/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 8/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 9/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 10/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 11/188) /content/p.mp4: 384x640 9 persons, 24.4ms\n",
            "video 1/1 (frame 12/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 13/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 14/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 15/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 16/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 17/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 18/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 19/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 20/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 21/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 22/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 23/188) /content/p.mp4: 384x640 9 persons, 24.4ms\n",
            "video 1/1 (frame 24/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 25/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 26/188) /content/p.mp4: 384x640 9 persons, 24.4ms\n",
            "video 1/1 (frame 27/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 28/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 29/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 30/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 31/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 32/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 33/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 34/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 35/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 36/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 37/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 38/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 39/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 40/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 41/188) /content/p.mp4: 384x640 8 persons, 26.8ms\n",
            "video 1/1 (frame 42/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 43/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 44/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 45/188) /content/p.mp4: 384x640 8 persons, 28.8ms\n",
            "video 1/1 (frame 46/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 47/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 48/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 49/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 50/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 51/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 52/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 53/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 54/188) /content/p.mp4: 384x640 7 persons, 24.4ms\n",
            "video 1/1 (frame 55/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 56/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 57/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 58/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 59/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 60/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 61/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 62/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 63/188) /content/p.mp4: 384x640 8 persons, 24.4ms\n",
            "video 1/1 (frame 64/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 65/188) /content/p.mp4: 384x640 8 persons, 24.4ms\n",
            "video 1/1 (frame 66/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 67/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 68/188) /content/p.mp4: 384x640 8 persons, 24.4ms\n",
            "video 1/1 (frame 69/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 70/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 71/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 72/188) /content/p.mp4: 384x640 8 persons, 25.1ms\n",
            "video 1/1 (frame 73/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 74/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 75/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 76/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 77/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 78/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 79/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 80/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 81/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 82/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 83/188) /content/p.mp4: 384x640 8 persons, 24.4ms\n",
            "video 1/1 (frame 84/188) /content/p.mp4: 384x640 8 persons, 24.4ms\n",
            "video 1/1 (frame 85/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 86/188) /content/p.mp4: 384x640 8 persons, 24.4ms\n",
            "video 1/1 (frame 87/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 88/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 89/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 90/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 91/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 92/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 93/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 94/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 95/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 96/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 97/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 98/188) /content/p.mp4: 384x640 10 persons, 24.6ms\n",
            "video 1/1 (frame 99/188) /content/p.mp4: 384x640 10 persons, 24.4ms\n",
            "video 1/1 (frame 100/188) /content/p.mp4: 384x640 10 persons, 24.7ms\n",
            "video 1/1 (frame 101/188) /content/p.mp4: 384x640 10 persons, 25.6ms\n",
            "video 1/1 (frame 102/188) /content/p.mp4: 384x640 9 persons, 24.7ms\n",
            "video 1/1 (frame 103/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 104/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 105/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 106/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 107/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 108/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 109/188) /content/p.mp4: 384x640 8 persons, 32.9ms\n",
            "video 1/1 (frame 110/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 111/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 112/188) /content/p.mp4: 384x640 7 persons, 24.4ms\n",
            "video 1/1 (frame 113/188) /content/p.mp4: 384x640 7 persons, 39.9ms\n",
            "video 1/1 (frame 114/188) /content/p.mp4: 384x640 7 persons, 30.6ms\n",
            "video 1/1 (frame 115/188) /content/p.mp4: 384x640 7 persons, 29.7ms\n",
            "video 1/1 (frame 116/188) /content/p.mp4: 384x640 7 persons, 25.7ms\n",
            "video 1/1 (frame 117/188) /content/p.mp4: 384x640 7 persons, 33.9ms\n",
            "video 1/1 (frame 118/188) /content/p.mp4: 384x640 6 persons, 24.7ms\n",
            "video 1/1 (frame 119/188) /content/p.mp4: 384x640 6 persons, 25.5ms\n",
            "video 1/1 (frame 120/188) /content/p.mp4: 384x640 6 persons, 24.4ms\n",
            "video 1/1 (frame 121/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 122/188) /content/p.mp4: 384x640 6 persons, 24.4ms\n",
            "video 1/1 (frame 123/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 124/188) /content/p.mp4: 384x640 6 persons, 24.9ms\n",
            "video 1/1 (frame 125/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 126/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 127/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 128/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 129/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 130/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 131/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 132/188) /content/p.mp4: 384x640 5 persons, 24.4ms\n",
            "video 1/1 (frame 133/188) /content/p.mp4: 384x640 4 persons, 24.3ms\n",
            "video 1/1 (frame 134/188) /content/p.mp4: 384x640 4 persons, 24.4ms\n",
            "video 1/1 (frame 135/188) /content/p.mp4: 384x640 4 persons, 24.4ms\n",
            "video 1/1 (frame 136/188) /content/p.mp4: 384x640 4 persons, 24.3ms\n",
            "video 1/1 (frame 137/188) /content/p.mp4: 384x640 4 persons, 24.7ms\n",
            "video 1/1 (frame 138/188) /content/p.mp4: 384x640 4 persons, 24.4ms\n",
            "video 1/1 (frame 139/188) /content/p.mp4: 384x640 4 persons, 24.3ms\n",
            "video 1/1 (frame 140/188) /content/p.mp4: 384x640 4 persons, 24.3ms\n",
            "video 1/1 (frame 141/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 142/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 143/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 144/188) /content/p.mp4: 384x640 5 persons, 25.1ms\n",
            "video 1/1 (frame 145/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 146/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 147/188) /content/p.mp4: 384x640 4 persons, 24.3ms\n",
            "video 1/1 (frame 148/188) /content/p.mp4: 384x640 4 persons, 24.4ms\n",
            "video 1/1 (frame 149/188) /content/p.mp4: 384x640 4 persons, 24.3ms\n",
            "video 1/1 (frame 150/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 151/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 152/188) /content/p.mp4: 384x640 5 persons, 24.6ms\n",
            "video 1/1 (frame 153/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 154/188) /content/p.mp4: 384x640 5 persons, 24.4ms\n",
            "video 1/1 (frame 155/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 156/188) /content/p.mp4: 384x640 6 persons, 27.9ms\n",
            "video 1/1 (frame 157/188) /content/p.mp4: 384x640 6 persons, 24.4ms\n",
            "video 1/1 (frame 158/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 159/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 160/188) /content/p.mp4: 384x640 5 persons, 24.4ms\n",
            "video 1/1 (frame 161/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 162/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 163/188) /content/p.mp4: 384x640 5 persons, 24.3ms\n",
            "video 1/1 (frame 164/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 165/188) /content/p.mp4: 384x640 6 persons, 24.3ms\n",
            "video 1/1 (frame 166/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 167/188) /content/p.mp4: 384x640 7 persons, 24.3ms\n",
            "video 1/1 (frame 168/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 169/188) /content/p.mp4: 384x640 8 persons, 24.3ms\n",
            "video 1/1 (frame 170/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 171/188) /content/p.mp4: 384x640 10 persons, 24.4ms\n",
            "video 1/1 (frame 172/188) /content/p.mp4: 384x640 10 persons, 24.4ms\n",
            "video 1/1 (frame 173/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 174/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 175/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 176/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 177/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 178/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 179/188) /content/p.mp4: 384x640 10 persons, 24.3ms\n",
            "video 1/1 (frame 180/188) /content/p.mp4: 384x640 10 persons, 24.4ms\n",
            "video 1/1 (frame 181/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 182/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 183/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 184/188) /content/p.mp4: 384x640 9 persons, 25.8ms\n",
            "video 1/1 (frame 185/188) /content/p.mp4: 384x640 9 persons, 24.7ms\n",
            "video 1/1 (frame 186/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 187/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "video 1/1 (frame 188/188) /content/p.mp4: 384x640 9 persons, 24.3ms\n",
            "Speed: 3.5ms preprocess, 24.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Bot Sort\n",
        "results = model.track(source=\"/content/p.mp4\", persist=True, save=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac4d9f33-2def-448b-888f-f5e4fcd71d54",
      "metadata": {
        "id": "ac4d9f33-2def-448b-888f-f5e4fcd71d54"
      },
      "outputs": [],
      "source": [
        "# Byte Track\n",
        "results = model.track(\"/content/p.mp4\", persist=True, show=True, classes=0, tracker=\"bytetrack.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e21c6d-c0e6-4ec9-9049-d8f6b473460c",
      "metadata": {
        "id": "70e21c6d-c0e6-4ec9-9049-d8f6b473460c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}